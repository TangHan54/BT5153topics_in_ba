# Week 4 Bag of Words
## [One-hot encoder](https://medium.com/@athif.shaffy/one-hot-encoding-of-text-b69124bef0a7)

- Pros
    - simple
    - suitable for parallel computing
- Cons
    - All words are independent and lost the meaning of the sentence. (The vector representation is in binary form, therefore no frequency information is taken into account.)
    - Out-of-Vocabulary problem
    - dimension is the size of the vocabulary

## [Bag-of-Words](https://machinelearningmastery.com/gentle-introduction-bag-words-model/)
A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:

A vocabulary of known words.
A measure of the presence of known words.

- Pros
    - Simple
    - Surprisingly effective
    - Fast
- Cons
    - Order of words does not matter
    - Meaning: Cannot capture syntactic/semantic information
    - Sparsity: High dimensionality. The vocabulary requires careful design, most specifically in order to manage the size, which impacts the sparsity of the document representations.
## [N-gram model](https://machinelearningmastery.com/gentle-introduction-bag-words-model/)
- Pros
    - considered word order
- Cons
    - Sparsity: Vocab size is very huge
    - Meaning: Can not capture syntactic/semantic information
    - Only able to incorporate limited word order information
 ## [TF-IDF](https://machinelearningmastery.com/gentle-introduction-bag-words-model/)
1. Build vocab i.e., set of all the words in the corpus
2. Count the occurrence of words in each document
3. Use weighting scheme to determine the value
    - TF(w) = number of times term w appears in a document/Total number of terms in the document
    - IDF(w) = log(total number of documents / number of documents with the term w in it)
    - The final weight is TF(w) * IDF(w)
- Intuitive logic:
    - Capture the importances of a word to document in a corpus
    - Importance of words is proportionally to the number of times a word appears
    - Importance of words is inversely proportionally to the document containing the word
 
## Semantic Representation
### Reduce highdimensionality
- LSA: Latent SemanticAnalysis
- Topic Models
### [Word Embeddings](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/)
- Definition
    - A continuous vector representation of word
    - Syntax and semantics information may be encoded into embeddings space.
- Advantage
    - **Low-dimensional** and **dense** word vectors make the application of neural network on NLP possible
    - Word vectors will be related: similar words may be close to each other in the vector space.
- Disadvantage:
    - Sparsity
    - High dimensions
    - Need dimensionality reduction
- Improvement
    - Apply dimension reduction
    - word2vec model